{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: sentence_transformers in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (2.2.0)\r\n",
      "Requirement already satisfied: tqdm in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from sentence_transformers) (4.64.0)\r\n",
      "Requirement already satisfied: sentencepiece in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from sentence_transformers) (0.1.96)\r\n",
      "Requirement already satisfied: torchvision in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from sentence_transformers) (0.12.0)\r\n",
      "Requirement already satisfied: scipy in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from sentence_transformers) (1.8.0)\r\n",
      "Requirement already satisfied: nltk in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from sentence_transformers) (3.7)\r\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from sentence_transformers) (1.11.0)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from sentence_transformers) (4.19.2)\r\n",
      "Requirement already satisfied: huggingface-hub in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from sentence_transformers) (0.6.0)\r\n",
      "Requirement already satisfied: numpy in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from sentence_transformers) (1.22.3)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from sentence_transformers) (1.1.0)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (4.2.0)\r\n",
      "Requirement already satisfied: requests in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.27.1)\r\n",
      "Requirement already satisfied: filelock in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.7.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.4.24)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.9)\r\n",
      "Requirement already satisfied: joblib in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from nltk->sentence_transformers) (1.1.0)\r\n",
      "Requirement already satisfied: click in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from nltk->sentence_transformers) (8.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.0.12)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.10.8)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.26.9)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from scikit-learn->sentence_transformers) (3.1.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from torchvision->sentence_transformers) (9.1.0)\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting konlpy\r\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 19.4 MB 4.8 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting lxml>=4.1.0\r\n",
      "  Downloading lxml-4.8.0-cp38-cp38-macosx_10_14_x86_64.whl (4.5 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 4.5 MB 5.6 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting JPype1>=0.7.0\r\n",
      "  Downloading JPype1-1.3.0-cp38-cp38-macosx_10_9_x86_64.whl (381 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 381 kB 3.8 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.6 in /Users/aidev_yyc_mac/opt/anaconda3/envs/yyc/lib/python3.8/site-packages (from konlpy) (1.22.3)\r\n",
      "Installing collected packages: lxml, JPype1, konlpy\r\n",
      "Successfully installed JPype1-1.3.0 konlpy-0.6.0 lxml-4.8.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers\n",
    "!pip install konlpy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "드론 활용 범위도 점차 확대되고 있다. 최근에는 미세먼지 관리에 드론이 활용되고 있다.\n",
    "서울시는 '미세먼지 계절관리제' 기간인 지난달부터 오는 3월까지 4개월간 드론에 측정장치를 달아 미세먼지 집중 관리를 실시하고 있다.\n",
    "드론은 산업단지와 사업장 밀집지역을 날아다니며 미세먼지 배출 수치를 점검하고, 현장 모습을 영상으로 담는다.\n",
    "영상을 통해 미세먼지 방지 시설을 제대로 가동하지 않는 업체와 무허가 시설에 대한 단속이 한층 수월해질 전망이다.\n",
    "드론 활용에 가장 적극적인 소방청은 광범위하고 복합적인 재난 대응 차원에서 드론과 관련 전문인력 보강을 꾸준히 이어가고 있다.\n",
    "지난해 말 기준 소방청이 보유한 드론은 총 304대, 드론 조종 자격증을 갖춘 소방대원의 경우 1,860명이다.\n",
    "이 중 실기평가지도 자격증까지 갖춘 ‘드론 전문가’ 21명도 배치돼 있다.\n",
    "소방청 관계자는 \"소방드론은 재난현장에서 영상정보를 수집, 산악ㆍ수난 사고 시 인명수색·구조활동,\n",
    "유독가스·폭발사고 시 대원안전 확보 등에 활용된다\"며\n",
    "\"향후 화재진압, 인명구조 등에도 드론을 활용하기 위해 연구개발(R&D)을 하고 있다\"고 말했다.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "품사 태깅 10개만 출력 : [('\\n', 'Foreign'), ('드론', 'Noun'), ('활용', 'Noun'), ('범위', 'Noun'), ('도', 'Josa'), ('점차', 'Noun'), ('확대', 'Noun'), ('되고', 'Verb'), ('있다', 'Adjective'), ('.', 'Punctuation')]\n",
      "명사 추출 : 드론 활용 범위 점차 확대 최근 미세먼지 관리 드론 활용 서울시 미세먼지 계절 관리제 기간 지난달 개 월간 드론 측정 장치 달 미세먼지 집중 관리 실시 드론 산업 단지 사업 밀집 지역 미세먼지 배출 수치 점검 현장 모습 영상 영상 통해 미세먼지 방지 시설 제대로 가동 업체 무허가 시설 대한 단속 한층 전망 드론 활용 가장 적극 소방청 복합 재난 대응 차원 드론 관련 전문 인력 보강 어가 지난해 말 기준 소방청 보유 드론 총 드론 조종 자격증 소방대 경우 명 이 중 실기 평가 지도 자격증 드론 전문가 명도 배치 소방청 관계자 소방 드론 재난 현장 영상 정보 수집 산악 수난 사고 시 인명 수색 구조 활동 유독가스 폭발사고 시 대원 안전 확보 등 활용 며 향후 화재 진압 인명구조 등 드론 활용 위해 연구개발 고 말\n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "\n",
    "tokenized_doc = okt.pos(doc)\n",
    "tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'])\n",
    "\n",
    "print('품사 태깅 10개만 출력 :',tokenized_doc[:10])\n",
    "print('명사 추출 :',tokenized_nouns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram 개수 : 222\n",
      "trigram 다섯개만 출력 : ['가동 업체' '가동 업체 무허가' '가장 적극' '가장 적극 소방청' '경우 실기']\n"
     ]
    }
   ],
   "source": [
    "n_gram_range = (2, 3)\n",
    "\n",
    "count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
    "candidates = count.get_feature_names_out()\n",
    "\n",
    "print('trigram 개수 :',len(candidates))\n",
    "print('trigram 다섯개만 출력 :',candidates[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['algorithm analyzes training', 'learning algorithm generalize', 'learning machine learning', 'learning algorithm analyzes', 'algorithm generalize training']\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Max Sum Similarity\n",
    "데이터 쌍 사이의 최대 합 거리는 데이터 쌍 간의 거리가 최대화되는 데이터 쌍으로 정의됩니다. 여기서의 의도는 후보 간의 유사성을 최소화하면서 문서와의 후보 유사성을 극대화하고자 하는 것입니다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
    "    # 문서와 각 키워드들 간의 유사도\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings,\n",
    "                                            candidate_embeddings)\n",
    "\n",
    "    # 코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 pick.\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # 각 키워드들 중에서 가장 덜 유사한 키워드들간의 조합을 계산\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}